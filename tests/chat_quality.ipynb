{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.setup_envs import load_local_settings\n",
    "from tests.tests_case import TESTS_CASES\n",
    "load_local_settings()\n",
    "\n",
    "from configs.openai_client import AzureOpenAIClient\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from configs.system_prompt import DEFAULT_PROMPT\n",
    "from configs.settings import vector_store\n",
    "from azure.ai.evaluation import (\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    ResponseCompletenessEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_MODEL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_with_excel_formatting(df):\n",
    "    df = df.rename(columns={\"query_id\": \"test_id\"})  # Caso ainda esteja usando query_id\n",
    "    ordered_cols = [\"test_id\", \"intent_score\", \"completeness_score\", \"task_adherence_score\", \"query\", \"chatbot_response\"]\n",
    "    other_cols = [col for col in df.columns if col not in ordered_cols]\n",
    "    df = df[ordered_cols + other_cols]\n",
    "\n",
    "    # Gera Excel com formatação e tabela\n",
    "    output_file = \"test_results_azure_case_04_hybrid_approach_no_hyde.xlsx\"\n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"results\", index=False)\n",
    "        workbook  = writer.book\n",
    "        sheet     = writer.sheets[\"results\"]\n",
    "\n",
    "        # Formatação do cabeçalho\n",
    "        header_fmt = workbook.add_format({ # type: ignore\n",
    "            'bold': True,\n",
    "            'text_wrap': True,\n",
    "            'valign': 'top',\n",
    "            'fg_color': '#D7E4BC',\n",
    "            'border': 1\n",
    "        })\n",
    "        for col_idx, col in enumerate(df.columns):\n",
    "            sheet.write(0, col_idx, col, header_fmt)\n",
    "\n",
    "        for col_idx, col in enumerate(df.columns):\n",
    "            series = df[col].astype(str)\n",
    "            if not series.empty:\n",
    "                raw_max = series.map(len).max()\n",
    "                try:\n",
    "                    max_data_len = int(raw_max)\n",
    "                except Exception:\n",
    "                    max_data_len = int(raw_max.values[0]) if hasattr(raw_max, 'values') else 0 # type: ignore\n",
    "            else:\n",
    "                max_data_len = 0\n",
    "            header_len = len(col)\n",
    "            col_width = max(max_data_len, header_len) + 2\n",
    "            sheet.set_column(col_idx, col_idx, col_width)\n",
    "\n",
    "        max_row, max_col = df.shape\n",
    "        last_col_letter = xl_col_to_name(max_col - 1) # type: ignore\n",
    "        table_range = f\"A1:{last_col_letter}{max_row+1}\"\n",
    "        sheet.add_table(table_range, {\n",
    "            'columns': [{'header': hdr} for hdr in df.columns],\n",
    "            'style': 'Table Style Medium 9'\n",
    "        })\n",
    "    print(f\"✅ Avaliação salva em '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "data_frame:pd.DataFrame = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5603c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_test_case(query: str):\n",
    "    embedding = AzureOpenAIClient.create_embedding(input_text=query)\n",
    "    filters = {}  # ou setar class_code\n",
    "    docs = vector_store.similarity_search_with_score_by_vector(embedding, k=8, filter=filters)\n",
    "    used_documents = \",\".join([f\"{doc.metadata['file_id']}[{score:.4f}]\" for doc, score in docs])\n",
    "\n",
    "    # 3) Monta contexto e metadata\n",
    "    context = [doc.page_content for doc, score in docs]\n",
    "\n",
    "    # 4) Cria prompt final e chama chatbot\n",
    "    assistant_prompt = (\n",
    "        f\"{DEFAULT_PROMPT}\\nBaseado nas seguintes informações: {context}\\n\"\n",
    "        f\"Responda à seguinte pergunta: {query}\"\n",
    "    )\n",
    "    assistant_response, raw = AzureOpenAIClient.create_completion(prompt=assistant_prompt)\n",
    "\n",
    "    return assistant_response, used_documents\n",
    "\n",
    "async def evaluate_with_azure():\n",
    "    print(\"Iniciando avaliação com Azure...\")\n",
    "    intent_evaluator      = IntentResolutionEvaluator(model_config=model_config)\n",
    "    completeness_evaluator = ResponseCompletenessEvaluator(model_config=model_config)\n",
    "    adherence_evaluator    = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "    for idx, case in enumerate(TESTS_CASES, start=1):\n",
    "        query_id = f\"test_{idx}\"\n",
    "        query = case[\"query\"]\n",
    "        expected = case[\"expected_answer\"]\n",
    "        print(f\"Avaliando teste: {query_id}\")\n",
    "        print(f\"  Pergunta: {query}\")\n",
    "\n",
    "        # 1) executa chat\n",
    "        response, used_docs = await execute_test_case(query)\n",
    "        # 2) avalia via Azure\n",
    "        intent  = intent_evaluator(query=query, response=response)\n",
    "        completeness = completeness_evaluator(response=response, ground_truth=expected)\n",
    "        adherence   = adherence_evaluator(query=query, response=response)\n",
    "\n",
    "        # 3) coleta resultados\n",
    "        results.append({\n",
    "            \"test_id\": query_id,\n",
    "            \"query\": query,\n",
    "            \"chatbot_response\": response,\n",
    "            \"expected_answer\": expected,\n",
    "            \"used_documents\": used_docs,\n",
    "            \"intent_score\": intent.get(\"intent_resolution\"),\n",
    "            \"completeness_score\": completeness.get(\"response_completeness\"),\n",
    "            \"task_adherence_score\": adherence.get(\"task_adherence\"),\n",
    "            \"intent_explanation\": intent.get(\"intent_resolution_reason\"),\n",
    "            \"completeness_explanation\": completeness.get(\"response_completeness_reason\"),\n",
    "            \"task_adherence_explanation\": adherence.get(\"task_adherence_reason\"),\n",
    "        })\n",
    "\n",
    "    data_frame = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_with_azure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_with_excel_formatting(data_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
